# LLM Benchmark

1. [ChatGPT - Jack of all trades, master of none](http://arxiv.org/pdf/2302.10724v4)
2. [Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering](http://arxiv.org/pdf/2506.00985v1)
3. [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/pdf/2506.21876)
4. [H2HTalk - Evaluating Large Language Models as Emotional Companion](http://arxiv.org/pdf/2507.03543v1)
5. [How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection](https://arxiv.org/pdf/2301.07597)
6. [How Persuasive is Your Context?](http://arxiv.org/pdf/2509.17879v1)
7. [PertEval - Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations](https://arxiv.org/pdf/2405.19740)

## LLM Agent Benchmark
1. [AgentBench - Evaluating LLMs as Agents](http://arxiv.org/pdf/2308.03688v2)
2. [BOLAA - BENCHMARKING AND ORCHESTRATING LLM-AUGMENTED AUTONOMOUS AGENTS](https://arxiv.org/pdf/2308.05960)
3. [GAIA - A Benchmark for General AI Assistants](https://arxiv.org/pdf/2311.12983)

### LLM Agent Benchmark Survey
1. [Evaluating LLM-based Agents for Multi-Turn Conversations - A Survey](http://arxiv.org/pdf/2503.22458v1)
2. [Survey on Evaluation of LLM-based Agents](https://arxiv.org/pdf/2503.16416)

### LLM GUI Agent Benchmark
1. [ASSISTGUI - Task-Oriented Desktop Graphical User Interface Automation](https://arxiv.org/pdf/2312.13108)
2. [AppAgent - Multimodal Agents as Smartphone Users](http://arxiv.org/pdf/2312.13771v2)
3. Benchmarking Mobile Device Control Agents Across Diverse Configuration
4. [OSWorld - Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments](http://arxiv.org/pdf/2404.07972v2)

### LLM Science Agent Benchmark
1. [FinResearchBench - A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](http://arxiv.org/pdf/2507.16248v1)

### LLM Tool Usage Benchmark
1. [API-Bank - A Comprehensive Benchmark for Tool-Augmented LLMs](https://aclanthology.org/2023.emnlp-main.187.pdf)
2. [ComplexFuncBench - Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario](http://arxiv.org/pdf/2501.10132v1)
3. [Gorilla - Large Language Model Connected with Massive APIs](https://arxiv.org/pdf/2305.15334)
4. [ToolQA - A Dataset for LLM Question Answering with External Tools](https://arxiv.org/pdf/2306.13304)

### LLM Web Agent Benchmark
1. [BrowseComp - A Simple Yet Challenging Benchmark for Browsing Agents](http://arxiv.org/pdf/2504.12516v1)
2. [BrowseComp-ZH - Benchmarking Web Browsing Ability of Large Language Models in Chinese](http://arxiv.org/pdf/2504.19314v2)
3. [WebArXiv - Evaluating Multimodal Agents on Time-Invariant arXiv Tasks](https://arxiv.org/pdf/2507.00938)

## LLM Benchmark Framework
1. [Fluid Language Model Benchmarking](http://arxiv.org/pdf/2509.11106v1)
2. [LiveBench - A Challenging, Contamination-Free LLM Benchmark](https://arxiv.org/pdf/2406.19314)
3. [PromptBench - A Unified Library for Evaluation of Large Language Models](https://arxiv.org/pdf/2312.07910)

## LLM Benchmark Methodology
1. [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](http://arxiv.org/pdf/2507.02856v1)
2. [Beyond the Leaderboard - Understanding Performance Disparities in Large Language Models via Model Diffing](http://arxiv.org/pdf/2509.18792v1)
3. [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](http://arxiv.org/pdf/2507.15707v1)
4. [Pretraining on the Test Set Is No Longer All You Need - A Debate-Driven Approach to QA Benchmarks](http://arxiv.org/pdf/2507.17747v1)
5. [Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above](http://arxiv.org/pdf/2502.14127v2)

## LLM Benchmark Score Prediction
1. [Look Before you Leap - Estimating LLM Benchmark Scores from Descriptions](http://arxiv.org/pdf/2509.20645v1)

## LLM Benchmark Survey
1. [A Survey on Evaluation of Large Language Models](https://arxiv.org/pdf/2307.03109)
2. [A Survey on Large Language Model Benchmarks](http://arxiv.org/pdf/2508.15361v1)
3. 大语言模型评测综述

## LLM Benchmark against Knowledge Leakage
1. [LASTINGBENCH - Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/pdf/2506.21614)

## LLM Code Benchmark
1. [Copilot Evaluation Harness - Evaluating LLM-Guided Software Programming](https://arxiv.org/pdf/2402.14261)

### LLM Code Generation Benchmarking
1. [AlgoTune - Can Language Models Speed Up General-Purpose Numerical Programs?](http://arxiv.org/pdf/2507.15887v1)
2. [CodeBLEU - a Method for Automatic Evaluation of Code Synthesis](https://arxiv.org/pdf/2009.10297)

### LLM Code Understanding Benchmarking
1. [AlgoSimBench - Identifying Algorithmically Similar Problems for Competitive Programming](http://arxiv.org/pdf/2507.15378v1)

## LLM Long Context and Memory Benchmark
1. [Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions](http://arxiv.org/pdf/2507.05257v1)
2. [Evaluating Very Long-Term Conversational Memory of LLM Agents](http://arxiv.org/pdf/2402.17753v1)
3. [LOOM-Scope - a comprehensive and efficient LOng-cOntext Model evaluation framework](http://arxiv.org/pdf/2507.04723v1)
4. [LongBench v2 - Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](http://arxiv.org/pdf/2412.15204v2)
5. [MADial-Bench - Towards Real-world Evaluation of Memory-Augmented Dialogue Generation](http://arxiv.org/pdf/2409.15240v2)
6. [MemSim - A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants](http://arxiv.org/pdf/2409.20163v1)
7. [StoryBench - A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns](https://arxiv.org/pdf/2506.13356)

### LLM Episodic Memory Benchmark
1. [Episodic Memories Generation and Evaluation Benchmark for Large Language Models](http://arxiv.org/pdf/2501.13121v1)

### LLM Long Context Benchmark
1. [BABILong - Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack](http://arxiv.org/pdf/2406.10149v2)
2. [In Search of Needles in a 11M Haystack - Recurrent Memory Finds What LLMs Miss](http://arxiv.org/pdf/2402.10790v2)
3. [LIFBench - Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios](http://arxiv.org/pdf/2411.07037v3)

### LLM Multimodal Memory Benchmark
1. [MMRC - A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation](http://arxiv.org/pdf/2502.11903v2)

## LLM Professional Knowledge Benchmark
1. [M3Exam - A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](http://arxiv.org/pdf/2306.05179v2)

### LLM Machine Learning Engineering Benchmark
1. [MLE-BENCH - EVALUATING MACHINE LEARNING AGENTS ON MACHINE LEARNING ENGINEERING](https://arxiv.org/pdf/2410.07095)

### LLM Mathematical Benchmark
1. [CO-Bench - Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization](https://www.arxiv.org/pdf/2504.04310)
2. [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](http://arxiv.org/pdf/2507.15855v2)

### LLM Medical Benchmark
1. [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](http://arxiv.org/pdf/2509.10843v1)

### LLM Science Benchmark
1. [Humanity's Last Exam](http://arxiv.org/pdf/2501.14249v7)
2. [The Ever-Evolving Science Exam](http://arxiv.org/pdf/2507.16514v1)

#### LLM Physical Benchmark
1. [PHYBench - Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/pdf/2504.16074)

## LLM RAG Benchmark
1. [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](http://arxiv.org/pdf/2508.03644v1)
2. [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](http://arxiv.org/pdf/2505.04847v1)
3. CRAG - Comprehensive RAG Benchmark
4. RAGBench - Explainable Benchmark for Retrieval-Augmented Generation Systems
5. [TARGET - Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/pdf/2505.11545)
6. [TP-RAG - Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning](http://arxiv.org/pdf/2504.08694v1)

### LLM RAG Benchmarking Survey
1. [Evaluation of Retrieval-Augmented Generation - A Survey](http://arxiv.org/pdf/2405.07437v2)

## LLM Reasoning Benchmark
1. [Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps](http://arxiv.org/pdf/2011.01060v2)
2. [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/pdf/2505.16998)
3. [GroundCocoa - A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models](https://aclanthology.org/2025.naacl-long.420.pdf)
4. [LLMThinkBench - Towards Basic Math Reasoning and Overthinking in Large Language Models](http://arxiv.org/pdf/2507.04023v1)
5. [LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench](http://arxiv.org/pdf/2409.13373v1)
6. [Language Models Do Not Follow Occam's Razor - A Benchmark for Inductive and Abductive Reasoning](http://arxiv.org/pdf/2509.03345v1)
7. [MuSiQue - Multihop Questions via Single-hop Question Composition](http://arxiv.org/pdf/2108.00573v3)
8. [PlanBench - An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](http://arxiv.org/pdf/2206.10498v4)
9. [Recitation over Reasoning - How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?](https://arxiv.org/pdf/2504.00509)

## LLM Structure Knowledge Understanding Benchmark
1. [SKA-Bench - A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](http://arxiv.org/pdf/2507.17178v1)

### LLM Table Understanding Benchmark
1. [Evaluating Generative Models for Tabular Data - Novel Metrics and Benchmarking](http://arxiv.org/pdf/2504.20900v1)
2. [MMTU - A Massive Multi-Task Table Understanding and Reasoning Benchmark](http://arxiv.org/pdf/2506.05587v2)

## LLM Theory of Mind Benchmark
1. [ToMBench - Benchmarking Theory of Mind in Large Language Models](http://arxiv.org/pdf/2402.15052v2)

## LLM-as-a-Judge
1. [Analyzing Uncertainty of LLM-as-a-Judge - Interval Evaluations with Conformal Prediction](http://arxiv.org/pdf/2509.18658v1)
2. [CHATEVAL - TOWARDS BETTER LLM-BASED EVALUATORS THROUGH MULTI-AGENT DEBATE](https://arxiv.org/pdf/2308.07201)
3. [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](http://arxiv.org/pdf/2507.17015v1)
4. [G-EVAL - NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/pdf/2303.16634)
5. [Large Language Models are Diverse Role-Players for Summarization Evaluation](https://arxiv.org/pdf/2303.15078)
6. [Large Language Models are not Fair Evaluators](http://arxiv.org/pdf/2305.17926v2)
7. One Token to Fool LLM-as-a-Judge
8. Training an LLM-as-a-Judge Model - Pipeline, Insights, and Practical Lessons
9. [Trust or Escalate - LLM Judges with Provable Guarantees for Human Agreement](http://arxiv.org/pdf/2407.18370v1)
10. [TrustJudge - Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](http://arxiv.org/pdf/2509.21117v1)

### LLM Meta Benchmark
1. [Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate](https://arxiv.org/pdf/2401.16788)
2. [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/pdf/2306.05685)

### LLM-Based Semantic Text Comparison
1. [BERTScore - Evaluating Text Generation with BERT](http://arxiv.org/pdf/1904.09675v3)
2. [GPTScore - Evaluate as You Desire](http://arxiv.org/pdf/2302.04166v2)
3. [Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks](http://arxiv.org/pdf/1908.10084v1)

### LLM-as-a-Judge Survey
1. [A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594)

## MLLM Benchmark
1. [A Survey on Benchmarks of Multimodal Large Language Models](https://arxiv.org/pdf/2408.08632)
2. [A Survey on Multimodal Benchmarks - In the Era of Large AI Models](https://arxiv.org/pdf/2409.18142)
3. [ConMe - Rethinking Evaluation of Compositional Reasoning for Modern VLMs](http://arxiv.org/pdf/2406.08164v3)
4. [MMMU - A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf)

### MLLM Benchmarking on Specialized Tasks
#### MLLM Material Characterization Benchmarking
1. [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](http://arxiv.org/pdf/2509.09307v1)

## NLU Benchmark
1. [Beyond the Imitation Game - Quantifying and extrapolating the capabilities of language models](https://arxiv.org/pdf/2206.04615)
2. [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/pdf/2507.01633)
3. [Drivel-ology - Challenging LLMs with Interpreting Nonsense with Depth](http://arxiv.org/pdf/2509.03867v1)
4. [GLUE - A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1804.07461)
5. [MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/2009.03300)
6. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](https://aclanthology.org/D16-1264.pdf)
7. [SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems](https://dl.acm.org/doi/pdf/10.5555/3454287.3454581)

### Metaphor Understanding Benchmarking
1. [Metaphor and Large Language Models - When Surface Features Matter More than Deep Understanding](http://arxiv.org/pdf/2507.15357v1)

## Personalized LLM Benchmark
1. [DO LLMS RECOGNIZE YOUR PREFERENCES? EVALUATING PERSONALIZED PREFERENCE FOLLOWING IN LLMS](https://arxiv.org/pdf/2502.09597)
2. [LaMP - When Large Language Models Meet Personalization](https://arxiv.org/pdf/2304.11406)
3. [UserBench - An Interactive Gym Environment for User-Centric Agents](http://arxiv.org/pdf/2507.22034v1)
